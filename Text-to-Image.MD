# Text-to-image Generation 

## Diffusion-based 

*[ICML 2021; OpenAI ] **---GLIDE---** GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models \[[PDF](https://arxiv.org/pdf/2112.10741.pdf), [Code](https://github.com/openai/glide-text2im)\]

[arxiv 2022; Microsoft] Vector Quantized Diffusion Model for Text-to-Image Synthesis \[[PDF](https://arxiv.org/pdf/2111.14822.pdf), [Code](https://github.com/cientgu/VQ-Diffusion)\]

[CVPR 2022; SUNY] Towards Language-Free Training for Text-to-Image Generation \[[PDF](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Towards_Language-Free_Training_for_Text-to-Image_Generation_CVPR_2022_paper.pdf), [Code](https://github.com/drboog/Lafite)\]

[ECCV 2022; UIUC ] Compositional Visual Generation with Composable Diffusion Models \[[PDF](https://arxiv.org/pdf/2206.01714.pdf), [Code](https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch)\]

[arxiv 2022; ByteDance] CLIP-GEN: Language-Free Training of a Text-to-Image Generator with CLIP \[[PDF](https://arxiv.org/pdf/2203.00386.pdf), [Code](https://github.com/HFAiLab/clip-gen)\]

[arxiv 2022; OpenAI ] Hierarchical Text-Conditional Image Generation with CLIP Latents \[[PDF](https://arxiv.org/pdf/2204.06125.pdf), Code\]

*[CVPR 2022] **---LDM---** High-Resolution Image Synthesis with Latent Diffusion Models \[[PDF](https://arxiv.org/pdf/2112.10752.pdf), [Code](https://github.com/CompVis/latent-diffusion)\]

*[arxiv 2022; Goole] **---Imagen---**  Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding \[[PDF](https://arxiv.org/pdf/2205.11487.pdf), Code\]

[arxiv 2023.01] Simple diffusion: End-to-end diffusion for high resolution images [[PDF](https://arxiv.org/pdf/2301.11093.pdf) ]

## GAN/VAE/Transformer-based 

[ICML 2021; OpenAI ] Zero-Shot Text-to-Image Generation \[[PDF](https://arxiv.org/pdf/2102.12092.pdf), [Code 3](https://github.com/YoadTew/zero-shot-image-to-text)\]

[CVPR 2021; Google ] Cross-Modal Contrastive Learning for Text-to-Image Generation \[[PDF](https://arxiv.org/pdf/2101.04702.pdf), [Code](https://github.com/google-research/xmcgan_image_generation)\]

[KDD, 2021; Alibaba ] **---M6---**  M6 : A Chinese Multimodal Pretrainer \[[PDF](https://arxiv.org/pdf/2103.00823.pdf), Code\]

[arxiv 2021; Baidu] **---ERNIE-ViLG---** ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation \[[PDF](https://arxiv.org/pdf/2112.15283.pdf), Code\]

[ECCV 2022] **---DT2I---** DT2I: Dense Text-to-Image Generation from Region Descriptions \[[PDF](https://arxiv.org/pdf/2204.02035.pdf), Code\]

*[arxiv 2022; Meta ] **---Make-a-scene---** Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors \[[PDF](https://arxiv.org/pdf/2203.13131.pdf), [Code 3](https://github.com/CasualGANPapers/Make-A-Scene)\]

*[arxiv 2022; Google] **---Parti---** Scaling Autoregressive Models for Content-Rich Text-to-Image Generation \[[PDF](https://arxiv.org/pdf/2206.10789.pdf), Code\]

[arxiv 2022.12] Scalable Diffusion Models with Transformers [[PDF](https://arxiv.org/abs/2212.09748), [Page](https://www.wpeebles.com/DiT)]

[arxiv 2022; Tsinghua ] **---CogView2---** CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers \[[PDF](https://arxiv.org/pdf/2204.14217.pdf), [Code](https://github.com/THUDM/CogView2)\]

[ECCV 2022; Microsoft] **---NÜWA--** NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion \[[PDF](https://arxiv.org/pdf/2111.12417.pdf), code \]

[NIPS 2022; Microsoft] **---NÜWA-Infinity--** NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis \[[PDF](https://arxiv.org/pdf/2207.09814.pdf), code \]

*[arxiv 2023.1; Google] **---Muse---** Muse: Text-To-Image Generation via Masked Generative Transformers [[PDF](https://arxiv.org/abs/2301.00704), [Page](https://muse-model.github.io/)]

[arxiv 2023.1]Attribute-Centric Compositional Text-to-Image Generation [[PDF](https://arxiv.org/pdf/2301.01413.pdf)]

[arxiv 2023.01]**---StyleGAN-T---** StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis [[PDF](https://arxiv.org/abs/2301.09515), [Page](https://sites.google.com/view/stylegan-t/)]

# Generation & Super-resolution 

[TPAMI 2022; Google ] Image Super-Resolution via Iterative Refinement \[[PDF](https://arxiv.org/pdf/2104.07636.pdf), [Code](https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement)\]


[CVPR 2022; POSTECH ]Autoregressive Image Generation using Residual Quantization \[[PDF](https://arxiv.org/pdf/2203.01941.pdf), [Code](https://github.com/kakaobrain/rq-vae-transformer)\]

[SIGGRAPH 2022; Goolge ] **---Palette---** Palette: Image-to-Image Diffusion Models\[[PDF](https://arxiv.org/pdf/2111.05826.pdf), [Code](https://github.com/Janspiry/Palette-Image-to-Image-Diffusion-Models)\]

[arxiv 2022; Google] Cascaded Diffusion Models for High Fidelity Image Generation\[[PDF](https://arxiv.org/pdf/2106.15282.pdf), Code\]


## Scene 
[arxiv 2022.12]Generative Scene Synthesis via Incremental View Inpainting using RGBD Diffusion Models [[PDF](https://arxiv.org/pdf/2212.05993.pdf)]

[arxiv 2022.12]Benchmarking Spatial Relationships in Text-to-Image Generation [[PDF](https://arxiv.org/pdf/2212.10015.pdf)]


# Transformer Related 
[ICLR 2022, Google]**---ViT-VQGAN---** Vector-quantized Image Modeling with Improved VQGAN [[PDF](https://arxiv.org/abs/2110.04627)]

[CVPR 2021, HEIDELBERG] **---VQGAN---** Taming transformers for high-resolution image synthesis[[PDF](https://arxiv.org/abs/2012.09841), [Page](https://compvis.github.io/taming-transformers/), [code](https://github.com/CompVis/taming-transformers)]

## Study 
[arxiv 2023.02] A Pilot Evaluation of ChatGPT and DALL-E 2 on Decision Making and Spatial Reasoning [[PDF](https://arxiv.org/abs/2302.09068)]

[arxiv 2023.02] Exploring the Representation Manifolds of Stable Diffusion Through the Lens of Intrinsic Dimension [[PDF](https://arxiv.org/abs/2302.09301)]
